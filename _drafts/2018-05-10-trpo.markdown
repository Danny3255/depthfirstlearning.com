---
layout: post
title:  "Trust Region Policy Optimization"
date:   2018-05-10 18:50:23 -0400
categories: reinforcement-learning
author: surya,krishna
blurb: "TRPO is a scalable algorithm of optimizing policies in reinforcement learning by gradient descent. Because model-free algorithms, such as policy gradient methods, do not require access to a model of the environment and often enjoy better practical stability, they are straightforward to apply to new problems, but were previously not able to scale to large, nonlinear policies. TRPO represents a significant improvement over previous methods in this sense and has demonstrated widespread success.
feedback: true
---

(Thank you to ... for contribution to this guide.)

TODO: Change to be TRPO's navigable svg.
<div class="deps-graph">
  <iframe class="deps" src="/assets/infogan-deps.svg" width="200"></iframe>
  <div>Concepts used in InfoGAN. Click to navigate.</div>
</div>

# Why

Trust Region Policy Optimization, or TRPO, is an approach to reinforcement
learning that uses a trust region to constrain the size of the update step.
TODO: ADD MORE CONTEXT.

<br />

# 1 Policy Gradients
  **Motivation**: TODO

  **Topics**:
  1. Markov Decision Processes (MDP).
  2. REINFORCE.
  3. Continuous action spaces.
  4. On-policy & Off-Policy settings.
  5. Policy gradients.

  **Required Readings**:
  1. CS 294 Lecture 4 about Policy Gradient:
      1. [Slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf)
      2. [Video](https://www.youtube.com/watch?v=tWNpiNzWuO8&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=4)
  2. David Silver’s class slides about PG:
      1. [Slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
      2. [Video](https://www.youtube.com/watch?v=KHZVXao4qXs&index=7&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
  3. Sutton and Barto 2nd Edition, pages 265 - 273.
  4. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)

  **Optional Readings**:
  1. [John Schulman introduction at MLSS Cadiz](http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/2016-MLSS-RL.pdf)
  2. [Lecture on Variance Reduction for Policy Gradient](http://rll.berkeley.edu/deeprlcoursesp17/docs/lec6.pdf)
  3. [Introduction to policy gradient and motivations by Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)
  4. [Connection Between Importance Sampling and Likelihood Ratio](http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf)

  **Questions**:
  1. Why does policy gradient have such high variance?
  2. Why does the step size matter?
  3. What is the difference between Off-policy and On-policy?
  4. What is the advantage of policy gradients for continuous action spaces?

<br />

# 2 (Optional, not used in TRPO) Variance Reduction and Advantage Estimate
  **Motivation**: TODO

  **Topics**:
  1. Likelihood Ratio.
  2. Importance Sampling connection.
  3. Variance reduction: Baselines & Causality.
  4. Advantage Estimation.

  **Required Readings**:
  1. CS 294 Lecture 5 about Actor Critic Algorithms
      1. [Slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf)
      2. [Video](https://www.youtube.com/watch?v=PpVhtJn-iZI&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=5)
  2. George Tucker’s notes on Variance Reduction (TODO: Where is this?)

  **Optional Readings**:
  1. Sutton and Barto 2nd Edition, pages 273 - 275.
  2. [High-dimensional continuous control using generalized advantage estimation](https://arxiv.org/abs/1506.02438)
  3. [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)

  **Questions**: TODO

<br />

# 3 Fisher Information Matrix and Natural Gradient Descent
  **Motivation**: While gradient descent is able to solve many optimization problems,
  it suffers from a basic problem -- its performance is dependent on the particular
  parameterization chosen for the model. Natural gradient descent is a variant of
  gradient descent, which is invariant to model parameterization. This is achieved
  by multiplying gradient vectors by the inverse of the Fisher Information Matrix (also
  called "conditioning on the Fisher").
  The Fisher Information Matrix measures how much model predictions change with
  local changes to parameters.

  **Topics**:
  1. Fisher Information Matrix.
  2. Natural Gradient Descent.
  3. K-Fac.

  **Required Readings**:
  1. Matt Johnson’s Natural Gradient Descent and K-Fac Tutorial: Sections 1-7, Section A, Section B (TODO: Where is this?)
  2. [New insights and perspectives on the natural gradient method](https://arxiv.org/pdf/1412.1193.pdf): Sections 1-11.
  3. [Fisher Information Matrix](https://web.archive.org/web/20170807004738/https://hips.seas.harvard.edu/blog/2013/04/08/fisher-information/)

  **Optional Readings**:
  1. [8-page intro to natural gradients](http://ipvs.informatik.uni-stuttgart.de/mlr/wp-content/uploads/2015/01/mathematics_for_intelligent_systems_lecture12_notes_I.pdf)
  2. [Why Natural Gradient Descent / Amari and Douglas](http://www.yaroslavvb.com/papers/amari-why.pdf)
  3. [Natural Gradient Works Efficiently in Learning / Amari](https://personalrobotics.ri.cmu.edu/files/courses/papers/Amari1998a.pdf)

  **Questions**:
  1. How is the Fisher Matrix similar and different from the Hessian?
  2. How does Natural gradient descent compare to Newton’s method?
  3. Why is the Natural Gradient slow to compute?

<br />

# 4 Conjugate Gradients
  **Motivation**: TODO

  **Topics**:
  1. Solving system of linear equations.
  2. Efficiently computing matrix-vector products .
  3. Computational complexities of 2nd order methods.

  **Required Readings**:
  1. [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf): Section 7 to 9.
  2. Convex Optimization II by Stephen Boyd:
      1. [Lecture 12, from 37:10 to 1:05:00](https://www.youtube.com/watch?feature=player_embedded&v=cHVpwyYU_LY#t=2230)
      2. [Lecture 13, from 21:20 to 29:30](https://www.youtube.com/watch?feature=player_embedded&v=E4gl91l0l40#t=1266)

  **Optional Readings**:
  1. (TODO: I might have gotten this wrong. Please double check) Numerical Optimization by Nocedal, Wright: Section 5.1 "The linear conjugate gradient method," up through "A practical form of the conjugate gradient method"
  2. [Metacademy](https://metacademy.org/graphs/concepts/conjugate_gradient)

  **Questions**: TODO

<br />

# 5 Trust Region Methods
  **Motivation**: TODO

  **Topics**:
  1. Trust Regions and subproblems.
  2. Line Search Methods.

  **Required Readings**:
  1. [A friendly introduction to Trust Region Methods](https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods)
  2. Numerical Optimization by Nocedal, Wright: Chapter 2, Chapter 4, Section 4.1, 4.2

  **Optional Readings**:
  1. Nocedal and Wright, Numerical Optimization: Chapter 4, Section 4.3

  **Questions**: TODO

<br />

# 6 The Paper
  **Motivation**: Let's read the [paper](https://arxiv.org/abs/1502.05477). TODO: add more motivation if necessary.

  **Topics**:
  1. What is the problem that TRPO addresses?
  2. What are the bottlenecks to addressing that problem in the existing approaches when it debuted?
  3. Which of those bottlenecks does it effectively address? Why?

  **Required Readings**:
  1. [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)
  2. Deep Reinforcement Learning @Berkeley
      1. [Slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf)
      2. [Video](https://www.youtube.com/watch?v=ycCtmp4hcUs&feature=youtu.be&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3)

  **Optional Readings**:
  1. [Approximately Optimal Approximate Reinforcement Learning](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)
  2. [TRPO Tutorial](https://reinforce.io/blog/end-to-end-computation-graphs-for-reinforcement-learning/)
  3. [ACKTR](https://arxiv.org/abs/1708.05144)

  **Questions**:
  1. What is the connection between TRPO, PPO, and CPO?
  2. In practice, TRPO is really slow. What is the main computational bottleneck and how might we remove it?
  3. What is Policy Improvement and Monotic Improvement Theory?
  4. Why do we use conjugate gradient methods for optimization in TRPO? Can we exploit the fact the conjugate gradient optimization is differentiable?
  5. How is line search used in TRPO?
